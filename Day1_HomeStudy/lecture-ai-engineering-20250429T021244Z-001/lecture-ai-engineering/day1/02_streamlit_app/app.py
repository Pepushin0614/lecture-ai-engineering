# app.py
import streamlit as st
import ui                   # UIãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import llm                  # LLMãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import database             # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import metrics              # è©•ä¾¡æŒ‡æ¨™ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import data                 # ãƒ‡ãƒ¼ã‚¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import torch
from transformers import pipeline
from config import MODEL_NAME
from huggingface_hub import HfFolder

# --- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š ---
st.set_page_config(page_title="Rinna Chatbot", layout="wide")

st.markdown(
    """
    <style>
    /* èƒŒæ™¯ã‚’ãƒŸãƒ‡ã‚£ã‚¢ãƒ ãƒ‘ãƒ¼ãƒ—ãƒ«ã« */
    [data-testid="stApp"] {
        background-color: #9370db; /* ãƒŸãƒ‡ã‚£ã‚¢ãƒ ãƒ‘ãƒ¼ãƒ—ãƒ« */
    }

    /* ã‚¿ã‚¤ãƒˆãƒ«ã‚’å¯æ„›ã */
    .stMarkdown h1 {
        font-family: 'Comic Sans MS', cursive, sans-serif;
        font-size: 40px;
        color: #ff69b4;
    }


    /* æœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’é»’ã« */
    [data-testid="stMarkdownContainer"] p,
    [data-testid="stMarkdownContainer"] li,
    [data-testid="stMarkdownContainer"] span,
    [data-testid="stMarkdownContainer"] div {
        color: #ffffff;
    }
    </style>
    """,
    unsafe_allow_html=True
)

# --- åˆæœŸåŒ–å‡¦ç† ---
# NLTKãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆåˆå›èµ·å‹•æ™‚ãªã©ï¼‰
metrics.initialize_nltk()

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã€ä½œæˆï¼‰
database.init_db()

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒç©ºãªã‚‰ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’æŠ•å…¥
data.ensure_initial_data()

# LLMãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åˆ©ç”¨ï¼‰
# ãƒ¢ãƒ‡ãƒ«ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦å†åˆ©ç”¨
@st.cache_resource
# def load_model():
#    """LLMãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""
#    try:
#        device = "cuda" if torch.cuda.is_available() else "cpu"
#        pipe = pipeline(
#            model=MODEL_NAME,
#            device=device
#        st.success(f"ãƒ¢ãƒ‡ãƒ« '{MODEL_NAME}' ã®èª­ã¿è¾¼ã¿ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
#    except Exception as e:
#        st.error("GPUãƒ¡ãƒ¢ãƒªä¸è¶³ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ä¸è¦ãªãƒ—ãƒ­ã‚»ã‚¹ã‚’çµ‚äº†ã™ã‚‹ã‹ã€ã‚ˆã‚Šå°ã•ã„ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
#pipe = llm.load_model()



def load_model():
    """ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto"
        )

        st.success(f"ãƒ¢ãƒ‡ãƒ« '{MODEL_NAME}' ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚")
        return tokenizer, model

    except Exception as e:
        st.error(f"ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
        return None, None

# ã“ã“ã‚‚ä¿®æ­£ï¼ˆè‡ªåˆ†ã®load_modelã‚’å‘¼ã¶ï¼‰
tokenizer, model = llm.load_model()

# --- Streamlit ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ ---
st.title("ğŸŒ¸ ã‚Šã‚“ãªã¡ã‚ƒã‚“ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ ğŸŒ¸")
st.write("ã‚ãªãŸã®å¿ƒã«å¯„ã‚Šæ·»ã†ã€å°ã•ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ç–²ã‚ŒãŸã¨ãã€å¬‰ã—ã„ã¨ãã€ä½•ã§ã‚‚è©±ã—ã‹ã‘ã¦ã­ï¼ğŸ˜Š")
st.markdown("---")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
st.sidebar.title("ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³")
page = st.sidebar.radio(
    "ãƒšãƒ¼ã‚¸é¸æŠ",
   # ["ãƒãƒ£ãƒƒãƒˆ", "å±¥æ­´é–²è¦§", "ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç®¡ç†"]
    ["ãƒãƒ£ãƒƒãƒˆ"]
)


# --- ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ ---
if page == "ãƒãƒ£ãƒƒãƒˆ":
    if tokenizer and model:
        ui.display_chat_page(tokenizer, model)
    else:
        st.error("ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ã‚’åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

elif page == "å±¥æ­´é–²è¦§":
    ui.display_history_page()

elif page == "ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç®¡ç†":
    ui.display_data_page()

# --- ãƒ•ãƒƒã‚¿ãƒ¼ãªã©ï¼ˆä»»æ„ï¼‰ ---
st.sidebar.markdown("---")
st.sidebar.info("é–‹ç™ºè€…: [Pepushin0085]")