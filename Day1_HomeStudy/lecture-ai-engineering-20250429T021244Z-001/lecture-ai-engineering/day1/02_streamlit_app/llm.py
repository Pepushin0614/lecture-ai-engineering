# llm.py
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import streamlit as st
from config import MODEL_NAME

@st.cache_resource
def load_model():
    """ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_NAME,
            use_fast=False  # rinnaæ¨å¥¨
        )
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto"  # è‡ªå‹•ã§ãƒ‡ãƒã‚¤ã‚¹ãƒãƒƒãƒ”ãƒ³ã‚°
        )

        st.success(f"ãƒ¢ãƒ‡ãƒ« '{MODEL_NAME}' ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚")
        return tokenizer, model

    except Exception as e:
        st.error(f"ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
        return None, None

def generate_response(tokenizer, model, user_input):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã«å¯¾ã—ã¦å¿œç­”ã‚’ç”Ÿæˆã—ã€å…ƒæ°—ã¥ã‘ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä»˜åŠ ã—ã¦è¿”ã™"""
    if tokenizer is None or model is None:
        return "ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„ãŸã‚å¿œç­”ã§ãã¾ã›ã‚“ã€‚", 0

    import time
    start_time = time.time()

    try:
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’ãã®ã¾ã¾ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ï¼‰
        prompt = f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_input}<NL>ã‚·ã‚¹ãƒ†ãƒ : "

        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
        token_ids = tokenizer.encode(
            prompt,
            add_special_tokens=False,
            return_tensors="pt"
        ).to(model.device)

        # ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚Šå¿œç­”ç”Ÿæˆ
        with torch.no_grad():
            output_ids = model.generate(
                token_ids,
                do_sample=True,
                max_new_tokens=128,
                temperature=0.7,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.pad_token_id,
                bos_token_id=tokenizer.bos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )

        # å‡ºåŠ›ã®ãƒ‡ã‚³ãƒ¼ãƒ‰
        output = tokenizer.decode(
            output_ids[0][token_ids.shape[1]:],
            skip_special_tokens=True
        )
        output = output.replace("<NL>", "\n")  # <NL>ã‚’æ™®é€šã®æ”¹è¡Œã«å¤‰æ›

        end_time = time.time()
        response_time = end_time - start_time

        # --- ã“ã“ã§å›ºå®šã®å…ƒæ°—ã¥ã‘ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä»˜åŠ  ---
        greeting = "ğŸŒŸ ä»Šæ—¥ã‚‚é ‘å¼µã£ã¦ãã ã•ã„ã­ï¼\n\n"
        final_output = greeting + output.strip()

        return final_output, response_time

    except Exception as e:
        st.error(f"å¿œç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚", 0